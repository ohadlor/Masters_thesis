\chapter{Related Work}

In the context of \glspl{pomdp}, planning a globally optimal solution is intractable for all but the smallest problems~\cite{Papadimitriou87math}. As a result, recent efforts have focused on tree-based search algorithms to find asymptotically optimal solutions. POMCP~\cite{Silver10nips}, an extension of \gls{mcts} tailored for unobservable states, is one of the first particle tree based approaches to solving \glspl{pomdp}. Building upon POMCP, subsequent works introduced POMCPOW and PFT-DPW~\cite{Sunberg18icaps}. The former algorithm applies a weighted particle filter to approximate the belief, the latter algorithm propagates beliefs, not states, through the tree. These algorithms enable the handling of belief-dependent rewards, but they face scalability challenges in high-dimensional belief spaces due to particle representations. DESPOT~\cite{Ye17jair} and its successor~\cite{Garg19rss} assume that the value function is a linear function of the belief, as such its relevance is limited to such value functions that can be well approximated with piece-wise linear functions ($\alpha$-vectors), limiting their applicability, especially for information-theoretical rewards. In~\cite{Thomas21arxiv}, the authors propose $\rho$-POMCP$(\beta)$ which samples the belief as a \scare{bag} of state particles and propagates them via a particle filter. Finally~\cite{Fischer20icml} proposes IPFT to also address information-theoretical rewards for \glspl{pomdp}. Both~\cite{Thomas21arxiv,Fischer20icml} are hindered by the curse of dimensionality in high-dimensional states. At the core of these asymptotically optimal algorithms, the use of Hoeffding's inequality~\cite{Hoeffding63jasa} is required for the asymptotic convergence. Since~\cite{Hoeffding63jasa} , many papers~\cite{Bentkus08lmj, From13jscs, Cohen15as} have sought to improve upon Hoeffding's inequality.

Another class of algorithms seeks to plan with bounds that provide anytime deterministic guarantees~\cite{Barenboim23nips}. In~\cite{Sztyglic22iros}, the SITH-BSP algorithm utilizes formulated bounds on belief-dependent rewards to optimize policies more efficiently; however, it is primarily designed for scenarios with lower-dimensional belief spaces, and the bounds are specific to entropy-based rewards. AI-FSSS~\cite{Barenboim22ijcai} is another bound-based algorithm for belief dependent reward. It clusters observations in the tree into groups, performing the calculations on their mean to improve computational performance. Nevertheless, its applicability is primarily limited to lower-dimensional problems and the Boers estimator~\cite{Boers10fusion}. Finally~\cite{Yotam24tro} addresses the complexity associated with high-dimensional problems for information gain as the reward and also provides bounds for the expected reward.

The anytime planning algorithms discussed derive their bounds from probability theory, often from Jensen's inequality or Markov's inequality. Works in probability theory have sought to improve upon these bounds as well. In~\cite{Ramdas23arxiv} Markov's inequality is generalized for sets of random variables. Finally in~\cite{Castillo23arxiv} the author improves upon the Markov inequality by using the partial expectation as we have done, although still assumes that the random variable is non-negative.