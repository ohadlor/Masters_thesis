\section{Discussion}
\label{sec:discussion}
In this paper, we address the challenges associated with planning under uncertainty by introducing novel, tractable bounds on reward and value functions. We formulated and proved novel bounds utilizing the concept of partial expectation and developed probabilistic bounds that incorporate Hoeffding's inequality. Providing conditions for which they improve upon Hoeffding's inequality. These novel bounds offer a computationally efficient alternative to optimal solution calculations, providing simplification with guarantees.

We applied these bounds to various planning contexts, starting with bounding the expected reward relative to the observation space, pertinent to both state and belief-dependent rewards. Our approach extends to recursively bounding value functions and addresses the complexities of information-theoretic rewards. In high-dimensional state spaces, such as those found in active SLAM, we proposed methods for efficiently reasoning about future observation realizations by leveraging the structure of belief topologies. Finally we simulate planning in landmark-SLAM with bounds on the Q-function. To the best of our knowledge planning with non-parametric beliefs with landmark uncertainty has not been previously addressed, and more-so for the case of belief dependent rewards.

Future research should focus on optimizing the selection of the subset $\subSpace$ to achieve the tightest bounds and address guided MCTS~\cite{Silver10nips} with our bounds. Furthermore, leveraging the properties mentioned in \cref{sec:properties} for an adaptive algorithm shows promise. We look forwards to possible uses of the probability theory bounds in other fields.
